{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "#Creating the stacked LSTM Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# Calculating RMSE Performance\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "#setting figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY= [],[]\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a= dataset[i:(i+time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    df1 = data.reset_index()['Close']\n",
    "    df1= scaler.fit_transform(np.array(df1).reshape(-1, 1))\n",
    "    # SPLITTING INTO TRAIN AND TEST\n",
    "    #deciding the split size\n",
    "    trainSize = int(len(df1)*0.65)\n",
    "    testSize= len(df1)- trainSize\n",
    "\n",
    "    #Making the splits\n",
    "    trainData, testData = df1[0:trainSize, :],df1[trainSize:len(df1), :1 ] \n",
    "    time_step= 320\n",
    "    X_train, Y_train= create_dataset(trainData, time_step)\n",
    "    X_test, Y_test= create_dataset(testData, time_step)\n",
    "    #Before going into LSTM, we have to make our data 3D\n",
    "    X_train= X_train.reshape(X_train.shape[0], X_train.shape[1],1)\n",
    "    X_test= X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n",
    "    \n",
    "    #making the model\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(50,return_sequences=True,input_shape=(320,1)))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    \n",
    "    #Training the model\n",
    "    model.fit(X_train, Y_train, validation_data= (X_test, Y_test), epochs= 15, batch_size=128, verbose=1)\n",
    "    #Doing the predictions\n",
    "    train_predict= model.predict(X_train)\n",
    "    test_predict= model.predict(X_test)\n",
    "    # transforming back to original , to perform RMSE\n",
    "    train_predict= scaler.inverse_transform(train_predict)\n",
    "    test_predict= scaler.inverse_transform(test_predict)\n",
    "    \n",
    "    #Plotting the training and testing data\n",
    "    look_back= 320 #time stamp\n",
    "    #Training data\n",
    "    trainPredictPlot= np.empty_like(df1)\n",
    "    trainPredictPlot[:, :]= np.nan\n",
    "    trainPredictPlot[look_back: len(train_predict)+look_back, :]= train_predict\n",
    "\n",
    "    #Test Data\n",
    "    testPredictPlot= np.empty_like(df1)\n",
    "    testPredictPlot[:, :]= np.nan\n",
    "    testPredictPlot[len(train_predict)+(look_back*2) +1: len(df1) -1 , :]= test_predict\n",
    "\n",
    "    #Plotting the baseline and predictions\n",
    "\n",
    "    plt.plot(scaler.inverse_transform(df1))\n",
    "    plt.plot(trainPredictPlot)\n",
    "    plt.plot(testPredictPlot)\n",
    "    plt.show()\n",
    "    \n",
    "    x_input=testData[(len(testData)-320) :].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    \n",
    "    \n",
    "    #MAKING PREDICTIONS FOR THE NEXT 15 DAYS\n",
    "    \n",
    "    lst_output=[]\n",
    "    n_steps=320\n",
    "    i=0\n",
    "    while(i<15):\n",
    "\n",
    "        if(len(temp_input)>320):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            print(len(temp_input))\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "\n",
    "    day_new=np.arange(1,321)\n",
    "    day_pred=np.arange(321,336)\n",
    "\n",
    "    plt.plot(day_new,scaler.inverse_transform(df1[(len(df1)-320):]))\n",
    "    plt.plot(day_pred,scaler.inverse_transform(lst_output))\n",
    "    plt.show()     \n",
    "    # Combining the data into one plot , checking only the predicted plot\n",
    "    df3=df1.tolist()\n",
    "    df3.extend(lst_output)\n",
    "    df3=scaler.inverse_transform(df3[len(df1):]).tolist()\n",
    "    \n",
    "    plt.plot(df3)\n",
    "    plt.show()\n",
    "    \n",
    "    diff = []\n",
    "    for i in range(len(df3)-1):\n",
    "        diff.append(df3[i][0] - df3[i+1][0])\n",
    "\n",
    "    avg = sum(diff)/len(diff)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks = {\n",
    "    \"AXIS_BANK\": \"not defined\",\n",
    "    \"BAJAJ_FINANCE\" : \"not defined\",\n",
    "    \"BAJAJ_FINSERV\" : \"not defined\",\n",
    "    \"HDFC_BANK\" : \"not defined\",\n",
    "    \"ICICIBANK\" : \"not defined\",\n",
    "    \"INDUSIND_BANK\" : \"not defined\",\n",
    "    \"KOTAK_MAHINDRA\" : \"not defined\",\n",
    "    \"SBI\" : \"not defined\"  \n",
    "}\n",
    "\n",
    "for i in banks:\n",
    "    \n",
    "    path = \"./Dataset/BanksDataset/\"+i+\".csv\"\n",
    "    data = pd.read_csv(path)\n",
    "    if banks[i] == \"not defined\":\n",
    "        \n",
    "        x = predict(data)\n",
    "        banks[i] = x\n",
    "        if banks[i] < 0:\n",
    "            print(\"You can Invest in \"+i+\"!\")\n",
    "        elif banks[i] > 0:\n",
    "            print(\"You should not invest in \"+i+\"!\")\n",
    "        else:\n",
    "            print(\"You may Invest in \"+i+\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
